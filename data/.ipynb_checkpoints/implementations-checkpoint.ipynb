{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add small description, imports and comments once the functions are written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import section\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we present different functions used for solving linear regression. All of the functions use mean squared error, with or without the regularization term. Some of these functions are explicitly solving linear regression, while others are using different optimization algorithms (GD, SGD) in order to minimize the loss function. In each function parameters $X,y$ (in the code tx,y) represent the data. We assume that the first column of $X$ consists only of ones, which represents the constant attribute. There could be some additional parameters, such as initial parameter of regression, step size, maximal number of iterations etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve linear regression problem, we want to minimize loss function $L_{MSE}$ represented by a mean squared error: $L_{MSE}(w) = \\frac{1}{2N}\\left \\| y-Xw \\right \\|_2^2 $, where $N$ represents number of data points. In order to solve this optimization problem we will use gradient descent: $w_t = w_{t-1} - \\gamma \\triangledown L_{MSE}(w_{t-1}) = w_{t-1} + \\frac{\\gamma}{N} X^{T}(y-Xw_{t-1})$, for $t \\geq 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression using gradient descent\n",
    "# y:          vector of outputs (dimension N)\n",
    "# tx:         matrix of data (dimension N x D), such that tx[:, 0] = 1\n",
    "# initial_w:  vector (dimension D)\n",
    "# max_iters:  scalar\n",
    "# gamma:      scalar respresenting step size\n",
    "# return parameters w for the regression and loss\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w,max_iters, gamma, frequency):\n",
    "    N, D = tx.shape\n",
    "    \n",
    "    loss = []\n",
    "    \n",
    "    if frequency == 0:\n",
    "        frequency = max_iters-1\n",
    "    \n",
    "    # Iterations of gradient descent\n",
    "    w = initial_w\n",
    "    for i in range(max_iters):\n",
    "        grad = -np.dot(tx.T, (y - np.dot(tx,w))) / N\n",
    "        w = w - gamma * grad\n",
    "        # Calculating the loss\n",
    "        if i% frequency == 0:\n",
    "            r = y - np.dot(tx,w)\n",
    "            loss.append(np.dot(r,r) / (2*N))\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.07253193,  0.07551335,  0.011748  ,  0.00937741, -0.05389029]), [])\n",
      "(array([ 0.08116726,  0.0644661 ,  0.00402176,  0.02055971, -0.03601205]), [0.5140399764621802, 0.513602910574436, 0.5137761106951648, 0.5137215896537572, 0.5128077043533168, 0.5129398005276902, 0.5130857929093445, 0.5129522970980215, 0.5121770127594054, 0.5117541626451528, 0.5123891642069439, 0.5132665959260879, 0.5123306077251056, 0.5120028909913708, 0.5114074161916502, 0.5112155729459256, 0.5110625538582323, 0.5106226974927088, 0.5102837069937, 0.5097336961460485, 0.5094574099586243, 0.509360194086334, 0.5089063136997125, 0.5089710332555367, 0.5093649432878138, 0.5093308617089485, 0.5086516544232684, 0.5089900631107191, 0.5087589646050753, 0.5088663089897142, 0.5085354961468878, 0.5084866031227426, 0.5086026579651676, 0.5089292551154566, 0.5085003247949158, 0.5084024562226289, 0.5086787321009127, 0.5080011296714615, 0.5080077751542798, 0.5081662958405108, 0.5080093279398827, 0.5080647766732111, 0.5078680421169167, 0.507922915073161, 0.5079940594321841, 0.5078063665537408, 0.5077828072141242, 0.5076724491841085, 0.5074564635989, 0.5074457749631599, 0.5075874151377461, 0.5075153510198924, 0.5078020063449333, 0.5077964063897139, 0.5075637872745219, 0.5075771635582527, 0.5075297083002253, 0.5075841071853736, 0.507643025581109, 0.5073899485809681, 0.5073863808892487, 0.5073411265065296, 0.5073462826618487, 0.5073893846633595, 0.5073902712051894, 0.5075840839364096, 0.5078404548248767, 0.5079560833199427, 0.5080795418982054, 0.5078790169121722, 0.50791942073138, 0.5077157637806375, 0.5076474310394704, 0.5074828548214487, 0.5075005151129288, 0.5074369800347844, 0.5074367952060145, 0.5075015369620447, 0.507416506665388, 0.5075382013665622, 0.5074916449920054, 0.5073307549375738, 0.5073843077245589, 0.5075877066471396, 0.5078718199951929, 0.5076725603549842, 0.5075166223952012, 0.5074983707312996, 0.5076078444093558, 0.5074196281767664, 0.5074647993440404, 0.5075642157683725, 0.5074295648794437, 0.5075542783863726, 0.5077114370617842, 0.507909900866178, 0.5076615375942323, 0.5077490064367699, 0.5075160140852737, 0.5074033198394752, 0.5073208571598428, 0.5073288662938521, 0.507501477065456, 0.5077449384194972, 0.507629714928339, 0.5077430194003769, 0.5078849279179479, 0.5081913254742592, 0.5083758212520068, 0.5081695253678968, 0.5082340595693658, 0.508222561186223, 0.5080777234907159, 0.5080349651747987, 0.5079877752232204, 0.5081190116861152, 0.5083556230784738, 0.5077970867713172, 0.5078472929731113, 0.5076497852951999, 0.5073877458236439, 0.5075640391638814, 0.5074981782748265, 0.5077153142913319, 0.5076996201787165, 0.5080345419781259, 0.5082031136414012, 0.5079628765510968, 0.5079192734280578, 0.508067331097721, 0.5080400599775705, 0.5082068074753556, 0.5080404894279706, 0.5080975006395765, 0.5078742906813982, 0.507833754175553, 0.5078506386874663, 0.5080842411585231, 0.5083554455743081, 0.5084799384828872, 0.50853780114085, 0.5079384555471463, 0.5082488362955296, 0.5083676051028165, 0.5083670455275946, 0.5084998637744207, 0.5082843125676703, 0.5082908252065673, 0.508572744117071, 0.508301969467712, 0.5080675760033951, 0.5077255630231888, 0.5076196093014307, 0.5074367984899828, 0.5075142067779503, 0.5075723475399379, 0.5076513122219678, 0.5076419779067385, 0.5077153843593035, 0.5078586137544779, 0.5081435490901325, 0.5081803405720778, 0.5081094942056444, 0.5079886053883499, 0.5080384422619563, 0.5079011923445091, 0.5075810133444626, 0.5078231003889794, 0.5079961991071059, 0.5080864193951429, 0.507862109379568, 0.5081333561584265, 0.5078117622396371, 0.5078350310817893, 0.5077721608249586, 0.5080265599383579, 0.5085141944757897, 0.5085053676413691, 0.5086053418643793, 0.5088262441826396, 0.5083270223501897, 0.5084393011566379, 0.5083154687298627, 0.5081874646689113, 0.5079250506320521, 0.5081175456724685, 0.5078282445016462, 0.5079615604545519, 0.508075591260033, 0.5083699567676905, 0.5085111238348523, 0.5087269157574423, 0.5085961321395099, 0.5087316322980661, 0.5087476062266514, 0.5083691977803579, 0.5081400239117383, 0.5082590322153736, 0.5085911713590145, 0.5085262265022634, 0.5083231912249366, 0.5080830841746067, 0.5078810507443482, 0.508004731517204, 0.5080116389929704, 0.5079956819417388, 0.5081087567034456, 0.5085037307494066, 0.5080369338505759, 0.5080489366391331, 0.5086531759649847, 0.5085628574849772, 0.5089015172895256, 0.5084301577927118, 0.5080924741642772, 0.5082080505244011, 0.5084173644150295, 0.5082256533571118, 0.5083338610512955, 0.5081811018505153, 0.5080171740088785, 0.5077715018705422, 0.5077250453504297, 0.5076520536722716, 0.5076163869298905, 0.5075209066372618, 0.5077104403939077, 0.5078193862771097, 0.5079883106370181, 0.5078049348247583, 0.5078450844238028, 0.5077508239314642, 0.5078764455097293, 0.5082726954614138, 0.5081898569008506, 0.5081033607070492, 0.5082871510628149, 0.5082145577816012, 0.5083623607771215, 0.5086209263942778, 0.5085742513925307, 0.5086355343632727, 0.5083562930598905, 0.5080632539546366, 0.5080882790096645, 0.5081171711396283, 0.5079691960086248, 0.5080956184205094, 0.5081986799428991, 0.508270147518563, 0.5084117513502759, 0.5084427735001971, 0.5083506761283778, 0.508007757771675, 0.5079031412396514, 0.5077176028862564, 0.5077722936585177, 0.5077490036922716, 0.5078646443605682, 0.5079706715091039, 0.5079984720822821, 0.5080324264312479, 0.5081079577696217, 0.5081578651273638, 0.5081638977312486, 0.5081256988538952, 0.5085010587766485, 0.5085776516344405, 0.5083761883276593, 0.508420259069091, 0.5082883732668766, 0.5083230606559752, 0.5084128449152967, 0.5081245817572206, 0.5079866977924736, 0.5082269915085462, 0.5082784015557654, 0.5082030951175736, 0.5082567326422152, 0.5079048173056573, 0.5077908855111093, 0.5075265075308836, 0.5076247126146785, 0.5075972648260313, 0.5077031532608964, 0.5079454121647611, 0.5082008065809187, 0.5080210318202966, 0.5079871748567665, 0.5078761253291966, 0.508125613812684, 0.5082728918554011, 0.5082995259637126, 0.5081872097468941, 0.5085532675303938, 0.5083219000908078, 0.508085875368627, 0.5080265465104647, 0.5078255487524691, 0.5076172360030629, 0.5078507626789347, 0.5076654852576176, 0.5077400660049344, 0.5078428961974893, 0.5077395267507324, 0.507788598393933, 0.5077606884403839, 0.5077653052441015, 0.5075775918637419, 0.5078837321897076, 0.5078529073593971, 0.507841784155401, 0.5077809302650695, 0.507651126578638, 0.5076809239080569, 0.5076112858449622, 0.5078524227060739, 0.5080493539942637, 0.5080039993052066, 0.5080396871552219, 0.5079631772726636, 0.5079866162161901, 0.5079716906735872, 0.5078701574318958, 0.5077032683056935, 0.5077566462310061, 0.5075688946943585, 0.5077385933662347, 0.5076757881838134, 0.50753132693143, 0.5074917091692704, 0.5074500579234001, 0.5074156231122399, 0.5074952659163335, 0.507450988707518, 0.5074478835868617, 0.5075286856452728, 0.5076550499432835, 0.5077405874229212, 0.5076648564527801, 0.5075897281254914, 0.5073106068722893, 0.5074004328329282, 0.507346502211891, 0.507391001563228, 0.5076703812497277, 0.5075671728760038, 0.5076693064360832, 0.5077391311853396, 0.5080173847368897, 0.507843202570415, 0.5079792755323043, 0.5080225806836114, 0.5080113835114631, 0.5079492536269036, 0.5081184710824926, 0.5085116352627826, 0.5083061152413537, 0.5080966792909564, 0.5085931575660595, 0.5087963026006511, 0.5086742824373827, 0.5087507726030306, 0.5087014299847338, 0.5092058938134146, 0.509739707728608, 0.5098738345357204, 0.5097448936783349, 0.5095322307493119, 0.5090236641984577, 0.5091319293270604, 0.5091624977095541, 0.5089988637154566, 0.508342646413744, 0.5085365585088015, 0.5083553872402625, 0.5084935555806239, 0.5084708182522123, 0.5083481037014417, 0.5086416908419612, 0.5084428042470894, 0.5089988679101409, 0.5091110785293931, 0.5095779336748082, 0.5089217968312079, 0.5086806891937748, 0.5087580514794744, 0.5087653433018237, 0.5088081570177343, 0.5087925562694035, 0.508491605532491, 0.5081964298210541, 0.5078990166640978, 0.5078475774857532, 0.5078027449486022, 0.5080466421087488, 0.5079666699847963, 0.5076442027572245, 0.5075898823442725, 0.5074703186310252, 0.5076153766087005, 0.5075041409497606, 0.5074658241688896, 0.5073886085945264, 0.5075630222366257, 0.5074412652896805, 0.5073770545812074, 0.5075037888259404, 0.5075685624993886, 0.5077621116533358, 0.5077087505507395, 0.5079903756194243, 0.5079468802046017, 0.5085054232376987, 0.5082008700270397, 0.5080038002435285, 0.5079277119270977, 0.5078494215503097, 0.5078239744320222, 0.5078487930922538, 0.5078195724987393, 0.5077703963402764, 0.507813443586072, 0.5075469851527332, 0.5076778110765534, 0.5077340005069355, 0.507672340914237, 0.5078504782365085, 0.5080099693750665, 0.5081248751503624, 0.5082970803070017, 0.5085208357740643, 0.5085141746763707, 0.5083529932093515, 0.508070390367007, 0.5079158793458123, 0.5078214228349006, 0.5077596214493578, 0.507769422379692, 0.507805869058485, 0.5077564193221782, 0.5077097798157362, 0.507816567156486, 0.5078477386654776, 0.5076747737980681, 0.507632052784983, 0.5075050079377578, 0.5074764532834231, 0.5076009072951889, 0.5075807137734039, 0.5077282471726664, 0.5077071648154758, 0.507922979061633, 0.5079539696204488, 0.5082191625179387, 0.5080652547782183, 0.5078654882071216, 0.5080111875186544, 0.5083246423337647, 0.5081983731920884, 0.5085629205064178, 0.5087932305568194, 0.5083839428111374, 0.5081386511672227, 0.5083253133484616, 0.5082152295489223, 0.5082709723010037, 0.5081890319108696, 0.5085878727657033, 0.508164820060256, 0.5083282733802038, 0.5084410365480354, 0.5083716377027337, 0.5078537892094965, 0.5077579250852899, 0.5076576892640471, 0.5075347214399066, 0.5078680643276408, 0.5080070984903313, 0.5078931595047114, 0.5076696166252043, 0.5076556472095025, 0.507494485034755, 0.5074078401196439, 0.5074592942667279, 0.5073722783278507, 0.5074727992179688, 0.5074164868503984, 0.5075423845583925, 0.5075773232081935, 0.5074928747480895, 0.5074889335048044, 0.5079013160769792, 0.5078370423106879, 0.5076586717900444, 0.5076994764950452, 0.5076483168168184, 0.5078154314525805, 0.5080253059953929, 0.5077590807065231, 0.5078399125109447, 0.5077437526456013, 0.5076117551691854, 0.5075204884119429, 0.5077146261233998, 0.5076673517569582, 0.5076139388791552, 0.5079370392622398, 0.5079050942522062, 0.5077498485184367, 0.5075775126596181, 0.5075853104384429, 0.5078141421069703, 0.5075820080696959, 0.5075695769895847, 0.5074282611562191, 0.5074804842916244, 0.5075320425168307, 0.5075310546679184, 0.5076599596182472, 0.5076882208737095, 0.5074439859646288, 0.5077227825140126, 0.5075375659105376, 0.5075889298159103, 0.5079080853707633, 0.5080610900771986, 0.5078834826236355, 0.5078843992956661, 0.50777091004107, 0.5078222538770253, 0.5076043678830544, 0.5077843368460564, 0.5081465422433877, 0.5077698556796031, 0.5077338511295955, 0.5079984960326744, 0.507862407247033, 0.5076400039397619, 0.507628069823458, 0.5078266848832393, 0.5084240525142032, 0.5081453084791508, 0.5081693323108696, 0.508120559123796, 0.5079100569191188, 0.5078334222207739, 0.5077930308833736, 0.507595385758879, 0.5074052596144736, 0.507660650598231, 0.5078798504915827, 0.5079868308848591, 0.5079650058988445, 0.5078157703346775, 0.5078338954111998, 0.507872165740746, 0.5078902143366812, 0.507955928898694, 0.5076869309647708, 0.5077797883003399, 0.5079160131714648, 0.5081215660171255, 0.5080063698729717, 0.5084182052905653, 0.5084223215606112, 0.5083149983954931, 0.5082577481365034, 0.5081495410229278, 0.508038842642484, 0.5081314024187199, 0.5082166446576416, 0.5080389467198082, 0.5080067813039316, 0.5078192814832239, 0.5079515354852366, 0.5077731253162054, 0.5075909545835755, 0.5078788930108554, 0.5079046487797955, 0.5078938755050084, 0.5076877587544443, 0.5075658206566953, 0.5073871368674938, 0.5073283299972244, 0.5073308910874913, 0.5073233991376835, 0.5073488454540896, 0.5074916163083936, 0.5075151708763945, 0.5075676765037821, 0.507669271512394, 0.50761903057776, 0.5077294268552466, 0.5076552326572434, 0.5074865614943829, 0.5076675440907494, 0.5077118569550735, 0.5077715062383593, 0.5076883527775912, 0.5076061587520345, 0.5074970448794413, 0.5074013130211003, 0.5074000009579273, 0.5073348385313172, 0.5075304070158743, 0.507620642414874, 0.5075304002752247, 0.5074607830063017, 0.5075278615551818, 0.5075586236420128, 0.5075501389480932, 0.5076418640137853, 0.5075520197216957, 0.507633526345892, 0.5078003129384886, 0.507965776341657, 0.5079461051856528, 0.5080343520899406, 0.5081868405495119, 0.5083397030957111, 0.508241348243537, 0.5084235183349184, 0.5080825313218759, 0.5079411953091125, 0.5076671609318446, 0.5075882720220668, 0.5074990937096979, 0.5076392403208734, 0.5076139663689215, 0.5074123986849312, 0.5073924262134617, 0.5074378447991138, 0.5073894704115894, 0.5073873790271822, 0.5073571394156985, 0.5074009312705772, 0.5073893304321714, 0.5075211896101655, 0.5076269922240404, 0.5075064335752532, 0.5073094760147472, 0.507294819724152, 0.5073711341524033, 0.5074108851627155, 0.5075177622806915, 0.5076387713811188, 0.5075295923934139, 0.507439044025414, 0.5074816003694508, 0.5075299819565205, 0.5075510963280085, 0.507754939576329, 0.5078990629636098, 0.5081848759594958, 0.50818193766521, 0.5083056893441578, 0.5082257232199338, 0.5084562454153392, 0.5087008972294678, 0.5086148259541875, 0.5083861163521946, 0.50909783918374, 0.5088798078473306, 0.5087384457902628, 0.5086990808364845, 0.5090464702391135, 0.5084809755247366, 0.5083649921414967, 0.5081988271806598, 0.5085062134447854, 0.508287512432303, 0.5081456899160126, 0.5082316562339606, 0.5077917492675987, 0.5079818350612731, 0.5081869208002641, 0.508036704583734, 0.508069018690588, 0.5077549823881629, 0.5077999184244962, 0.5077721881627187, 0.5079141550354004, 0.5081440473000144, 0.5080976774137418, 0.5078328849102727, 0.5077301114867686, 0.5076833709622091, 0.5076052219022241, 0.5076685556574349, 0.507659362838819, 0.5075543092851098, 0.5076905268928754, 0.507769824011049, 0.5084277855664623, 0.5083434631178216, 0.5080751112317066, 0.5083364078061452, 0.5080452132391722, 0.50790101633251, 0.5077072175590601, 0.5077073561055021, 0.5076610018771404, 0.5076489469304528, 0.5077131596581557, 0.5077358255326334, 0.5079890309803363, 0.5082302562259653, 0.5083430167311583, 0.50807226009676, 0.5083295346900613, 0.5083627253418571, 0.5081810997542288, 0.5080592542253571, 0.5080624223533271, 0.5082462067371931, 0.5086083025970665, 0.508227905390456, 0.5075018339198, 0.5073857393242578, 0.5072494002568997, 0.5072515445412801, 0.5072716707594632, 0.5072671094856085, 0.5073159999601211, 0.5072517399753512, 0.5073193612723613, 0.5075467758201281, 0.5077826554998949, 0.5078174693940626, 0.5078986800664858, 0.5077361867140834, 0.5075790917972293, 0.507552242978332, 0.5074677910559409, 0.5075450544900844, 0.5076761393006511, 0.5077500107964484, 0.5077351707610228, 0.5077167667211735, 0.5077388618481111, 0.5079699084526271, 0.5080014694491086, 0.5082192989839112, 0.5087151983241678, 0.5086725927840842, 0.5084062928444285, 0.5082715436162865, 0.5083593763564236, 0.508236190276724, 0.5083597146574379, 0.507937388866706, 0.5080134354001133, 0.508042453979006, 0.50792589941304, 0.5078715799046626, 0.5080030303236488, 0.5082983853465707, 0.5080486923584295, 0.5080334133768787, 0.5080407715608125, 0.5079430318929242, 0.5079246768916549, 0.5082138677909845, 0.5080395968868341, 0.5080878984500866, 0.508104841260176, 0.5078338357747343, 0.5079330045133802, 0.5077531901705734, 0.5079211048513831, 0.5080802227932428, 0.5081356024576644, 0.5078797828002614, 0.5077914177123266, 0.5077861678069457, 0.5078442572738471, 0.508146894852103, 0.5080523282020162, 0.5080112577049608, 0.5082733932900495, 0.5082981143759314, 0.5084051568022498, 0.5079127619894969, 0.5078859562549909, 0.5078585042130338, 0.5077287423693481, 0.5075911059178371, 0.5077861208567696, 0.507757816616022, 0.5078217092766957, 0.5080547369178388, 0.5082297460623352, 0.5080822219845843, 0.5079898497703959, 0.5079055345594081, 0.5076926664005685, 0.5076555111271177, 0.5076893692942811, 0.5076853250901567, 0.5076436651586754, 0.5078163141625045, 0.5077397569425941, 0.5077373244992143, 0.5076304417339311, 0.5076785277818446, 0.5076085002050921, 0.5074616981629368, 0.5074229869024869, 0.5073406387468016, 0.5074412402455365, 0.5074239813382342, 0.5073110458618396, 0.5073169622923958, 0.5073544295769917, 0.507382628696717, 0.5074520563037551, 0.5074753452427355, 0.5077303638541738, 0.5076103869711678, 0.507893625418361, 0.5080338888977713, 0.5082657950694551, 0.508144302414461, 0.5083748012413426, 0.5082100525041613, 0.5079473595106538, 0.50790704842119, 0.507889523438865, 0.50785128629139, 0.5077682632958385, 0.5078826115434278, 0.5076999759448839, 0.5078263076194407, 0.5077898278315677, 0.5077562297988324, 0.5077629595727047, 0.5075026737359931, 0.5074024428777408, 0.5075078677544527, 0.5074973388832804, 0.5075488641347207, 0.5076001919305485, 0.5076420235734168, 0.5075714687946705, 0.507569858589424, 0.5073446782496788, 0.5074261844613086, 0.5075192403821155, 0.5074816560887349, 0.5073008086201833, 0.5074726818341845, 0.5072766561593692, 0.5074355276708539, 0.5074698582043008, 0.5073557025069055, 0.5072724113526719, 0.5073921729095014, 0.5073158264056934, 0.5074843412088818, 0.5075543119308074, 0.507400710742031, 0.507450694006623, 0.507535883100933, 0.5075179589385582, 0.5074535569085131, 0.5075399934089659, 0.5075190514163133, 0.5074392212856041, 0.5076102691347236, 0.5075466737133204, 0.5074418042889978, 0.507372582019106, 0.5073898878933585, 0.5074984324870923, 0.5074210439260859, 0.507328342521896, 0.5073142923472593, 0.5072500411867287, 0.5073829674000034, 0.5074482642433622, 0.5076324352207306, 0.5076932553663447, 0.5077566652362484, 0.5076324725113427, 0.5076447386710058, 0.5077483005757536, 0.5077947100398925, 0.5078585942618077, 0.5076921038942223, 0.5079003531058482, 0.5080413643351476, 0.5079362713598727, 0.5078486689974406, 0.5076702145483764, 0.5078556446488354, 0.5079764375089328, 0.5077683860067712, 0.5077562172203378, 0.5074889997272399, 0.5077946824353906, 0.5078798143854087, 0.5080868314210334, 0.5078619360923146, 0.5076300802475083, 0.5075791874177983, 0.5077586884695205, 0.5078397834461367, 0.5078590125014936, 0.5080233182978833, 0.5076432311789878, 0.50798186227841, 0.5080899264988186, 0.5083220740364642, 0.5080296360625037, 0.507915654705132, 0.5076964338580334, 0.5079982045580714, 0.5079487575574597, 0.5078719331695092, 0.5078437398666148, 0.5078537542277078, 0.5081002203335354, 0.508064189162941, 0.5078694507090393, 0.5077454634372417, 0.5081591171178288, 0.5080375482886682, 0.5079542048269754, 0.5081249180453487, 0.5082503904760268, 0.5084546351449856, 0.5080771828262928, 0.5079853941761888, 0.5079798750942205, 0.5080445641366067, 0.5080102369724492, 0.5083635389943199, 0.5083048553954693, 0.5077373011861825, 0.5077178685776292, 0.5078646780121324, 0.5075675850356387, 0.5074997001193544, 0.5075418301460732, 0.5075853751665648, 0.5078669189594427, 0.5079159547400572, 0.5078537857143679, 0.5079373337056998, 0.5081286410819654, 0.5080975375182388, 0.5077778552249014, 0.5076573117341207, 0.5077472381756345, 0.5075952246908687, 0.5076360826224683, 0.5077501933092862, 0.5076714361689798, 0.5076499034537284, 0.5075240096754198, 0.507676809255122, 0.5075453521162845, 0.5073335854472163, 0.5073323095459336, 0.5074754366660039, 0.5078658505117322, 0.5077889402430844, 0.5079608674549274, 0.5079843181647514, 0.5082180737558794, 0.5080677279084693, 0.508086882276292, 0.5082902718780091, 0.5082046015569286, 0.5077503049985741, 0.5078238650936591, 0.5077847540021854, 0.5077448994355146, 0.5076821097522719, 0.5076463617318712, 0.5077795666431765, 0.5077694201676177, 0.5074752400573402, 0.5073674189239599, 0.5074274677054099, 0.5074928257687986, 0.5076313834110179, 0.5076081660484223, 0.5075257263803377, 0.5075926944092942, 0.5075444947808245, 0.5076900456059132, 0.5076704746995109, 0.5075387481707458, 0.5076232114644719, 0.5075069024596297, 0.5077893265774331, 0.5077752043886739, 0.5077186115838708])\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "D = 5\n",
    "X = np.random.randn(N,D)\n",
    "y = np.random.randn(N)\n",
    "w = np.zeros(D)\n",
    "gamma = 0.0005\n",
    "print(least_squares_GD(y,X,w,1000000,gamma,0))\n",
    "print(least_squares_SGD(y,X,w,100000,gamma,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#Linear regression using stochastic gradient descent\n",
    "def least_squares_SGD(y, tx, initial_w,max_iters, gamma, frequency):\n",
    "    N, D = tx.shape\n",
    "    \n",
    "    loss = []\n",
    "    \n",
    "    # Iterations of stochastic gradient descent\n",
    "    w = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        k = random.randint(0,N-1)\n",
    "        grad = -(y[k]-np.dot(tx[k,:], w))*tx[k,:]\n",
    "        w = w - gamma * grad\n",
    "        # Calculating the loss\n",
    "        r = y - np.dot(tx,w)\n",
    "        loss.append(np.dot(r,r) / (2*N))\n",
    "    if frequency == 0:\n",
    "        loss = loss[len(loss)-1]\n",
    "    else:\n",
    "        loss = loss[::frequency]\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Least squares regression using normal equations\n",
    "def least_squares(y, tx):\n",
    "    w_opt = (np.linalg.inv((tx.T).dot(tx)).dot(tx.T)).dot(y)\n",
    "    y_pred = tx.dot(w_opt)\n",
    "    return (w_opt,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression implies that loss function is proportional to the sum of mean square error and regularization term that depends on $l_2$ squared norm of parameter $w$. Let $L_{rr}$ be the loss function, than: $L_{rr}(w) = \\frac{1}{2N}(\\left \\| y-Xw \\right \\|_2^2 + \\lambda \\left \\| w \\right \\|_2^2)$, where $N$ represents number of data points. Moreover,\n",
    "$\\triangledown L_{rr}(w) = \\frac{1}{N}(- X^{T}(y-Xw)+\\lambda w)$. After putting $\\triangledown L_{rr} = 0$, we obtain:\n",
    "$$w_* = (X^{T}X+\\lambda Id)^{-1}X^{T}y,$$\n",
    "where $w_*$ minimizes the loss function $L_{rr}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge regression using normal equations\n",
    "# y:          vector of outputs (dimension N)\n",
    "# tx:         matrix of data (dimension N x D), such that tx[:, 0] = 1\n",
    "#lambda_:     regularization parameter\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    N,D = tx.shape\n",
    "    \n",
    "    A = np.dot(tx.T, tx) + lambda_ * np.ones(D)\n",
    "    B = np.linalg.inv(A)\n",
    "    w = np.dot(np.dot(B,tx.T), y)\n",
    "    \n",
    "    # Calculating loss\n",
    "    r = y - np.dot(tx,w)\n",
    "    loss = (np.dot(r,r)+ lambda_ * np.dot(w,w)) / 2\n",
    "    \n",
    "    return w, loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns exp(x)/(1+exp(x))\n",
    "# x is scalar or numpy array\n",
    "def sigmoid(x):\n",
    "    tmp = np.exp(x)\n",
    "    return tmp/(1+tmp)\n",
    "\n",
    "#Logistic regression using SGD\n",
    "# y:          vector of outputs (dimension N)\n",
    "# tx:         matrix of data (dimension N x D), such that tx[:, 0] = 1\n",
    "# initial_w:  vector (dimension D)\n",
    "# max_iters:  scalar\n",
    "# gamma:      scalar respresenting step size\n",
    "# return parameters w for the regression and loss\n",
    "def logistic_regression(y, tx, initial_w,max_iters, gamma):\n",
    "    N, _ = tx.shape\n",
    "    w = initial_w\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        k = random.randint(0,N-1)\n",
    "        tmp = np.dot(tx[k,:],w)\n",
    "        grad = -y[k]*tx[k,:]+sigmoid(tmp)*tx[k,:]\n",
    "        w = w - gamma*grad\n",
    "    \n",
    "    tmp = np.dot(tx,w)\n",
    "    loss = - np.dot(y,tmp)+np.sum(np.log(1+np.exp(tmp)))\n",
    "    return (w, loss)\n",
    "\n",
    "\n",
    "def logistic_regression_GD(y, tx, initial_w,max_iters, gamma, frequency):\n",
    "    w = initial_w\n",
    "    \n",
    "    loss = []\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        tmp = np.dot(tx, w)\n",
    "        grad = np.dot((sigmoid(tmp) - y), tx)\n",
    "        w = w - gamma*grad\n",
    "        tmp = np.dot(tx,w)\n",
    "        loss.append(- np.dot(y,tmp)+np.sum(np.log(1+np.exp(tmp))))\n",
    "    if frequency == 0:\n",
    "        loss = loss[len(loss)-1]\n",
    "    else:\n",
    "        loss = loss[::frequency]\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularized logistic regression using SGD\n",
    "# y:          vector of outputs (dimension N)\n",
    "# tx:         matrix of data (dimension N x D), such that tx[:, 0] = 1\n",
    "# lambda:     scalar representing regularization parameter\n",
    "# initial_w:  vector (dimension D)\n",
    "# max_iters:  scalar\n",
    "# gamma:      scalar respresenting step size\n",
    "# return parameters w for the regression and loss\n",
    "def reg_logistic_regression(y, tx, lambda_ ,initial_w, max_iters, gamma):\n",
    "    N, _ = tx.shape\n",
    "    w = initial_w\n",
    "    loss = []\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        k = random.randint(0,N-1)\n",
    "        tmp = np.dot(tx[k,:],w)\n",
    "        grad = -y[k]*tx[k,:]+sigmoid(tmp)*tx[k,:]+lambda_*w\n",
    "        w = w - gamma*grad\n",
    "        ### \n",
    "        tmp = np.dot(tx,w)\n",
    "        loss.append(- np.dot(y,tmp)+np.sum(np.log(1+np.exp(tmp))))\n",
    "    if frequency == 0:\n",
    "        loss = loss[len(loss)-1]\n",
    "    else:\n",
    "        loss = loss[::frequency]\n",
    "    ### \n",
    "    #tmp = np.dot(tx,w)\n",
    "    #loss = - np.dot(y,tmp)+np.sum(np.log(1+np.exp(tmp)))\n",
    "    loss=0\n",
    "    return w, loss\n",
    "\n",
    "def reg_logistic_regression_GD(y, tx, lambda_ ,initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    \n",
    "    loss = []\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        tmp = np.dot(tx, w)\n",
    "        grad = np.dot((sigmoid(tmp) - y), tx) + lambda_*w\n",
    "        w = w - gamma*grad\n",
    "        ### \n",
    "        tmp = np.dot(tx,w)\n",
    "        loss.append(- np.dot(y,tmp)+np.sum(np.log(1+np.exp(tmp))))\n",
    "    if frequency == 0:\n",
    "        loss = loss[len(loss)-1]\n",
    "    else:\n",
    "        loss = loss[::frequency]\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-24b0f3bbec9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mw1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mw2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mw3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg_logistic_regression_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-a87661d11e62>\u001b[0m in \u001b[0;36mlogistic_regression\u001b[1;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Example for logistic regression\n",
    "\n",
    "N = 100\n",
    "D = 2\n",
    "\n",
    "category = lambda x : int(np.dot(x, np.ones(2)) > 0)\n",
    "\n",
    "X = np.random.rand(N,D)\n",
    "X = X - 0.5\n",
    "Y = np.array([category(x) for x in X])\n",
    "X += (np.ones((D, N))*Y).T\n",
    "\n",
    "\n",
    "X0 = X[np.argwhere(Y==0).flatten()]\n",
    "X1 = X[np.argwhere(Y==1).flatten()]\n",
    "\n",
    "const_attribute = np.ones((N,1))\n",
    "tx = np.concatenate((const_attribute, X), axis=1)\n",
    "initial_w = np.array([0,0,0])\n",
    "np.dot(tx, initial_w)\n",
    "\n",
    "print(tx.shape)\n",
    "w1 = logistic_regression(Y,tx, initial_w, 100000000, 0.001)\n",
    "w2 = reg_logistic_regression(Y,tx, 1, initial_w, 10000000, 0.001,0) \n",
    "w3 = reg_logistic_regression_GD(Y,tx, 1, initial_w, 10000000, 0.001,0) \n",
    "print(w1)\n",
    "print(w2)\n",
    "print(w3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = './data/train.csv.zip'\n",
    "path_test = './data/test.csv.zip'\n",
    "\n",
    "train_data = pd.read_csv(path_train, compression='zip')\n",
    "test_data = pd.read_csv(path_test, compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_met_phi</th>\n",
       "      <th>PRI_met_sumet</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>224999.500000</td>\n",
       "      <td>-49.023079</td>\n",
       "      <td>49.239819</td>\n",
       "      <td>81.181982</td>\n",
       "      <td>57.895962</td>\n",
       "      <td>-708.420675</td>\n",
       "      <td>-601.237051</td>\n",
       "      <td>-709.356603</td>\n",
       "      <td>2.373100</td>\n",
       "      <td>18.917332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010119</td>\n",
       "      <td>209.797178</td>\n",
       "      <td>0.979176</td>\n",
       "      <td>-348.329567</td>\n",
       "      <td>-399.254314</td>\n",
       "      <td>-399.259788</td>\n",
       "      <td>-692.381204</td>\n",
       "      <td>-709.121609</td>\n",
       "      <td>-709.118631</td>\n",
       "      <td>73.064591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>72168.927986</td>\n",
       "      <td>406.345647</td>\n",
       "      <td>35.344886</td>\n",
       "      <td>40.828691</td>\n",
       "      <td>63.655682</td>\n",
       "      <td>454.480565</td>\n",
       "      <td>657.972302</td>\n",
       "      <td>453.019877</td>\n",
       "      <td>0.782911</td>\n",
       "      <td>22.273494</td>\n",
       "      <td>...</td>\n",
       "      <td>1.812223</td>\n",
       "      <td>126.499506</td>\n",
       "      <td>0.977426</td>\n",
       "      <td>532.962789</td>\n",
       "      <td>489.338286</td>\n",
       "      <td>489.333883</td>\n",
       "      <td>479.875496</td>\n",
       "      <td>453.384624</td>\n",
       "      <td>453.389017</td>\n",
       "      <td>98.015662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.329000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.142000</td>\n",
       "      <td>13.678000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>162499.750000</td>\n",
       "      <td>78.100750</td>\n",
       "      <td>19.241000</td>\n",
       "      <td>59.388750</td>\n",
       "      <td>14.068750</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>1.810000</td>\n",
       "      <td>2.841000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.575000</td>\n",
       "      <td>123.017500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>224999.500000</td>\n",
       "      <td>105.012000</td>\n",
       "      <td>46.524000</td>\n",
       "      <td>73.752000</td>\n",
       "      <td>38.467500</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>2.491500</td>\n",
       "      <td>12.315500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024000</td>\n",
       "      <td>179.739000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.960000</td>\n",
       "      <td>-1.872000</td>\n",
       "      <td>-2.093000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>40.512500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>287499.250000</td>\n",
       "      <td>130.606250</td>\n",
       "      <td>73.598000</td>\n",
       "      <td>92.259000</td>\n",
       "      <td>79.169000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>83.446000</td>\n",
       "      <td>-4.593000</td>\n",
       "      <td>2.961000</td>\n",
       "      <td>27.591000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.561000</td>\n",
       "      <td>263.379250</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>75.349000</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>33.703000</td>\n",
       "      <td>-2.457000</td>\n",
       "      <td>-2.275000</td>\n",
       "      <td>109.933750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>349999.000000</td>\n",
       "      <td>1192.026000</td>\n",
       "      <td>690.075000</td>\n",
       "      <td>1349.351000</td>\n",
       "      <td>2834.999000</td>\n",
       "      <td>8.503000</td>\n",
       "      <td>4974.979000</td>\n",
       "      <td>16.690000</td>\n",
       "      <td>5.684000</td>\n",
       "      <td>2834.999000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.142000</td>\n",
       "      <td>2003.976000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1120.573000</td>\n",
       "      <td>4.499000</td>\n",
       "      <td>3.141000</td>\n",
       "      <td>721.456000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>3.142000</td>\n",
       "      <td>1633.433000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Id   DER_mass_MMC  DER_mass_transverse_met_lep  \\\n",
       "count  250000.000000  250000.000000                250000.000000   \n",
       "mean   224999.500000     -49.023079                    49.239819   \n",
       "std     72168.927986     406.345647                    35.344886   \n",
       "min    100000.000000    -999.000000                     0.000000   \n",
       "25%    162499.750000      78.100750                    19.241000   \n",
       "50%    224999.500000     105.012000                    46.524000   \n",
       "75%    287499.250000     130.606250                    73.598000   \n",
       "max    349999.000000    1192.026000                   690.075000   \n",
       "\n",
       "        DER_mass_vis       DER_pt_h  DER_deltaeta_jet_jet  DER_mass_jet_jet  \\\n",
       "count  250000.000000  250000.000000         250000.000000     250000.000000   \n",
       "mean       81.181982      57.895962           -708.420675       -601.237051   \n",
       "std        40.828691      63.655682            454.480565        657.972302   \n",
       "min         6.329000       0.000000           -999.000000       -999.000000   \n",
       "25%        59.388750      14.068750           -999.000000       -999.000000   \n",
       "50%        73.752000      38.467500           -999.000000       -999.000000   \n",
       "75%        92.259000      79.169000              0.490000         83.446000   \n",
       "max      1349.351000    2834.999000              8.503000       4974.979000   \n",
       "\n",
       "       DER_prodeta_jet_jet  DER_deltar_tau_lep     DER_pt_tot  ...  \\\n",
       "count        250000.000000       250000.000000  250000.000000  ...   \n",
       "mean           -709.356603            2.373100      18.917332  ...   \n",
       "std             453.019877            0.782911      22.273494  ...   \n",
       "min            -999.000000            0.208000       0.000000  ...   \n",
       "25%            -999.000000            1.810000       2.841000  ...   \n",
       "50%            -999.000000            2.491500      12.315500  ...   \n",
       "75%              -4.593000            2.961000      27.591000  ...   \n",
       "max              16.690000            5.684000    2834.999000  ...   \n",
       "\n",
       "         PRI_met_phi  PRI_met_sumet    PRI_jet_num  PRI_jet_leading_pt  \\\n",
       "count  250000.000000  250000.000000  250000.000000       250000.000000   \n",
       "mean       -0.010119     209.797178       0.979176         -348.329567   \n",
       "std         1.812223     126.499506       0.977426          532.962789   \n",
       "min        -3.142000      13.678000       0.000000         -999.000000   \n",
       "25%        -1.575000     123.017500       0.000000         -999.000000   \n",
       "50%        -0.024000     179.739000       1.000000           38.960000   \n",
       "75%         1.561000     263.379250       2.000000           75.349000   \n",
       "max         3.142000    2003.976000       3.000000         1120.573000   \n",
       "\n",
       "       PRI_jet_leading_eta  PRI_jet_leading_phi  PRI_jet_subleading_pt  \\\n",
       "count        250000.000000        250000.000000          250000.000000   \n",
       "mean           -399.254314          -399.259788            -692.381204   \n",
       "std             489.338286           489.333883             479.875496   \n",
       "min            -999.000000          -999.000000            -999.000000   \n",
       "25%            -999.000000          -999.000000            -999.000000   \n",
       "50%              -1.872000            -2.093000            -999.000000   \n",
       "75%               0.433000             0.503000              33.703000   \n",
       "max               4.499000             3.141000             721.456000   \n",
       "\n",
       "       PRI_jet_subleading_eta  PRI_jet_subleading_phi  PRI_jet_all_pt  \n",
       "count           250000.000000           250000.000000   250000.000000  \n",
       "mean              -709.121609             -709.118631       73.064591  \n",
       "std                453.384624              453.389017       98.015662  \n",
       "min               -999.000000             -999.000000        0.000000  \n",
       "25%               -999.000000             -999.000000        0.000000  \n",
       "50%               -999.000000             -999.000000       40.512500  \n",
       "75%                 -2.457000               -2.275000      109.933750  \n",
       "max                  4.500000                3.142000     1633.433000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['s', 'b'], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Prediction'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id: 250000\n",
      "Prediction: 2\n",
      "DER_mass_MMC: 108338\n",
      "DER_mass_transverse_met_lep: 101637\n",
      "DER_mass_vis: 100558\n",
      "DER_pt_h: 115563\n",
      "DER_deltaeta_jet_jet: 7087\n",
      "DER_mass_jet_jet: 68366\n",
      "DER_prodeta_jet_jet: 16593\n",
      "DER_deltar_tau_lep: 4692\n",
      "DER_pt_tot: 59042\n",
      "DER_sum_pt: 156098\n",
      "DER_pt_ratio_lep_tau: 5931\n",
      "DER_met_phi_centrality: 2829\n",
      "DER_lep_eta_centrality: 1002\n",
      "PRI_tau_pt: 59639\n",
      "PRI_tau_eta: 4971\n",
      "PRI_tau_phi: 6285\n",
      "PRI_lep_pt: 61929\n",
      "PRI_lep_eta: 4987\n",
      "PRI_lep_phi: 6285\n",
      "PRI_met: 87836\n",
      "PRI_met_phi: 6285\n",
      "PRI_met_sumet: 179740\n",
      "PRI_jet_num: 4\n",
      "PRI_jet_leading_pt: 86590\n",
      "PRI_jet_leading_eta: 8558\n",
      "PRI_jet_leading_phi: 6285\n",
      "PRI_jet_subleading_pt: 42464\n",
      "PRI_jet_subleading_eta: 8628\n",
      "PRI_jet_subleading_phi: 6286\n",
      "PRI_jet_all_pt: 103559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in train_data.columns:\n",
    "    print(f'{column}: {len(train_data[column].unique())}')\n",
    "\n",
    "train_data['PRI_jet_num'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "boson = train_data[train_data['Prediction']=='b']\n",
    "spiner = train_data[train_data['Prediction']=='s']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_balance(x,y):\n",
    "    labels = ['Bosons', 'Spiners']\n",
    "    sizes = [x,y]\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "    plt.show()\n",
    "\n",
    "plot_balance(boson.shape[0], spiner.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000,), (250000, 30))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_output_into_binary(y):\n",
    "    a, b = np.unique(y)\n",
    "    category = lambda x : int(x==a)\n",
    "    new_y = np.array([category(i) for i in y])\n",
    "    return new_y\n",
    "\n",
    "def add_const_attribute(X):\n",
    "    N, _ = X.shape\n",
    "    tx = np.concatenate((np.ones((N, 1)), X), axis=1)\n",
    "    return tx\n",
    "\n",
    "y = transform_output_into_binary(train_data['Prediction'].values)\n",
    "tx = add_const_attribute(train_data.drop(['Prediction', 'Id', 'PRI_jet_num'], axis=1).values)\n",
    "\n",
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2191.026\n",
      "690.075\n",
      "1343.0220000000002\n",
      "2834.9990000000003\n",
      "1007.503\n",
      "5973.979\n",
      "1015.69\n",
      "5.476\n",
      "2834.9990000000003\n",
      "1806.3580000000002\n",
      "19.726\n",
      "2.8280000000000003\n",
      "1000.0\n",
      "744.408\n",
      "4.996\n",
      "6.284\n",
      "534.271\n",
      "5.008\n",
      "6.284\n",
      "2842.5080000000003\n",
      "6.284\n",
      "1990.298\n",
      "2119.5730000000003\n",
      "1003.499\n",
      "1002.141\n",
      "1720.4560000000001\n",
      "1003.5\n",
      "1002.142\n",
      "1633.433\n",
      "[[1.         0.51914948 0.07485418 ... 0.99675137 0.99439501 0.06948372]\n",
      " [1.         0.52940358 0.09965294 ... 0.         0.         0.02829991]\n",
      " [1.         0.         0.23500634 ... 0.         0.         0.0270908 ]\n",
      " ...\n",
      " [1.         0.50408211 0.08770931 ... 0.         0.         0.02570782]\n",
      " [1.         0.49928709 0.02805782 ... 0.         0.         0.        ]\n",
      " [1.         0.         0.10543202 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    N,D = x.shape\n",
    "    \n",
    "    for i in range(1,D):\n",
    "        x_min = x[:,i].min()\n",
    "        x_max = x[:,i].max()\n",
    "        print(x_max-x_min)\n",
    "        x[:,i] = (x[:,i]-x_min)/(x_max-x_min)\n",
    "        \n",
    "normalize(tx)\n",
    "print(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### needs to be implemented\n",
    "### one issue that we have is that our data set is not balanced \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(tx, y)\n",
    "\n",
    "\n",
    "plot_balance(np.count_nonzero(y_train), np.count_nonzero(y_train==1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w):\n",
    "    p = sigmoid(np.dot(x,w))\n",
    "    print(p.max(), p.min())\n",
    "    return np.array([int(i>0.5) for i in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(187500, 30)\n",
      "(30,)\n",
      "(187500,)\n"
     ]
    }
   ],
   "source": [
    "initial_w = 0*np.ones(x_train.shape[1])\n",
    "max_iter = 1000000\n",
    "gamma = 0.000001\n",
    "print(x_train.shape)\n",
    "print(initial_w.shape)\n",
    "print(y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7a134e15238f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e59c9e861a05>\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "w = reg_logistic_regression(y_train, x_train, 0.1, initial_w, max_iter, gamma,0)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-3a26a6f46480>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mypredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-ae0ed7efa884>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(x, w)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-ae0ed7efa884>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "ypredict = predict(tx,w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = reg_logistic_regression(y_train, x_train, 0.1, w[0], max_iter, gamma/10,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6126940608231158 0.5368258273891988\n"
     ]
    }
   ],
   "source": [
    "ypredict = predict(tx,w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(187500, 90)\n"
     ]
    }
   ],
   "source": [
    "tx2 = np.concatenate((x_train, x_train*x_train, x_train*x_train*x_train), axis=1)\n",
    "print(tx2.shape)\n",
    "_, D = tx2.shape \n",
    "w = reg_logistic_regression(y_train, tx2, 0.01, 0*np.ones(D), max_iter, gamma,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9040193732822437 0.30417984780936713\n"
     ]
    }
   ],
   "source": [
    "ypredict = predict(tx2,w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123121"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypredict = predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187500"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
