{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add small description, imports and comments once the functions are written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import section\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we present different functions used for solving linear regression. All of the functions use mean squared error, with or without the regularization term. Some of these functions are explicitly solving linear regression, while others are using different optimization algorithms (GD, SGD) in order to minimize the loss function. In each function parameters $X,y$ (in the code tx,y) represent the data. We assume that the first column of $X$ consists only of ones, which represents the constant attribute. There could be some additional parameters, such as initial parameter of regression, step size, maximal number of iterations etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve linear regression problem, we want to minimize loss function $L_{MSE}$ represented by a mean squared error: $L_{MSE}(w) = \\frac{1}{2N}\\left \\| y-Xw \\right \\|_2^2 $, where $N$ represents number of data points. In order to solve this optimization problem we will use gradient descent: $w_t = w_{t-1} - \\gamma \\triangledown L_{MSE}(w_{t-1}) = w_{t-1} + \\frac{\\gamma}{N} X^{T}(y-Xw_{t-1})$, for $t \\geq 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression using gradient descent\n",
    "# y:          vector of outputs (dimension N)\n",
    "# tx:         matrix of data (dimension N x D), such that tx[:, 0] = 1\n",
    "# initial_w:  vector (dimension D)\n",
    "# max_iters:  scalar\n",
    "# gamma:      scalar respresenting step size\n",
    "# return parameters for the regression and loss\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w,max_iters, gamma):\n",
    "    N, D = tx.shape\n",
    "    \n",
    "    # Iterations of gradient descent\n",
    "    w = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        grad = -np.dot(tx.T, (y - np.dot(tx,w))) / N\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "    # Calculating the loss\n",
    "    r = y - np.dot(tx,w)\n",
    "    loss = np.dot(r,r) / (2*N)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.01917264,  0.02215317,  0.02065718,  0.00507561,  0.00614499]), 0.47324051066191636)\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "D = 5\n",
    "X = np.random.randn(N,D)\n",
    "y = np.random.randn(N)\n",
    "w = np.zeros(D)\n",
    "gamma = 0.0005\n",
    "print(least_squares_GD(y,X,w,10000,gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression using stochastic gradient descent\n",
    "def least_squares_SGD(y, tx, initial_w,max_iters, gamma):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Least squares regression using normal equations\n",
    "def least squares(y, tx):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression implies that loss function is proportional to the sum of mean square error and regularization term that depends on $l_2$ squared norm of parameter $w$. Let $L_{rr}$ be the loss function, than: $L_{rr}(w) = \\frac{1}{2N}(\\left \\| y-Xw \\right \\|_2^2 + \\lambda \\left \\| w \\right \\|_2^2)$, where $N$ represents number of data points. Moreover,\n",
    "$\\triangledown L_{rr}(w) = \\frac{1}{N}(- X^{T}(y-Xw)+\\lambda w)$. After putting $\\triangledown L_{rr} = 0$, we obtain:\n",
    "$$w_* = (X^{T}X+\\lambda Id)^{-1}X^{T}y,$$\n",
    "where $w_*$ minimizes the loss function $L_{rr}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge regression using normal equations\n",
    "# y:          vector of outputs (dimension N)\n",
    "# tx:         matrix of data (dimension N x D), such that tx[:, 0] = 1\n",
    "#lambda_:     regularization parameter\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    N,D = tx.shape\n",
    "    \n",
    "    A = np.dot(tx.T, tx) + lambda_ * np.ones(D)\n",
    "    B = np.linalg.inv(A)\n",
    "    w = np.dot(np.dot(B,tx.T), y)\n",
    "    \n",
    "    # Calculating loss\n",
    "    r = y - np.dot(tx,w)\n",
    "    loss = (np.dot(r,r)+ lambda_ * np.dot(w,w)) / 2\n",
    "    \n",
    "    return w, loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression using gradient descent or SGD\n",
    "def logistic_regression(y, tx, initial w,max_iters, gamma):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularized logistic regression using gradient descent or SGD\n",
    "def reg logistic regression(y, tx, lambda_ ,initial w, max_iters, gamma):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
